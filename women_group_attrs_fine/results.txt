10/11/2024 23:11:51 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:27394
W1011 23:11:52.476000 22965575968448 torch/distributed/run.py:779] 
W1011 23:11:52.476000 22965575968448 torch/distributed/run.py:779] *****************************************
W1011 23:11:52.476000 22965575968448 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1011 23:11:52.476000 22965575968448 torch/distributed/run.py:779] *****************************************
10/11/2024 23:11:58 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
10/11/2024 23:11:58 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
10/11/2024 23:11:58 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
10/11/2024 23:11:58 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.
10/11/2024 23:11:58 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
10/11/2024 23:11:58 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:672] 2024-10-11 23:11:58,717 >> loading configuration file config.json from cache at /iitjhome/m23csa016/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/51c47430f97dd7c74aa1fa6825e68a813478097f/config.json
[WARNING|modeling_rope_utils.py:379] 2024-10-11 23:11:58,718 >> Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
[INFO|configuration_utils.py:739] 2024-10-11 23:11:58,719 >> Model config Qwen2VLConfig {
  "_name_or_path": "Qwen/Qwen2-VL-7B-Instruct",
  "architectures": [
    "Qwen2VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2_vl",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.0",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "in_chans": 3,
    "model_type": "qwen2_vl",
    "spatial_patch_size": 14
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2214] 2024-10-11 23:11:58,968 >> loading file vocab.json from cache at /iitjhome/m23csa016/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/51c47430f97dd7c74aa1fa6825e68a813478097f/vocab.json
[INFO|tokenization_utils_base.py:2214] 2024-10-11 23:11:58,968 >> loading file merges.txt from cache at /iitjhome/m23csa016/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/51c47430f97dd7c74aa1fa6825e68a813478097f/merges.txt
[INFO|tokenization_utils_base.py:2214] 2024-10-11 23:11:58,969 >> loading file tokenizer.json from cache at /iitjhome/m23csa016/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/51c47430f97dd7c74aa1fa6825e68a813478097f/tokenizer.json
[INFO|tokenization_utils_base.py:2214] 2024-10-11 23:11:58,969 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2214] 2024-10-11 23:11:58,969 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2214] 2024-10-11 23:11:58,969 >> loading file tokenizer_config.json from cache at /iitjhome/m23csa016/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/51c47430f97dd7c74aa1fa6825e68a813478097f/tokenizer_config.json
Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
[INFO|tokenization_utils_base.py:2478] 2024-10-11 23:11:59,254 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|image_processing_base.py:375] 2024-10-11 23:12:00,026 >> loading configuration file preprocessor_config.json from cache at /iitjhome/m23csa016/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/51c47430f97dd7c74aa1fa6825e68a813478097f/preprocessor_config.json
[INFO|image_processing_base.py:375] 2024-10-11 23:12:00,317 >> loading configuration file preprocessor_config.json from cache at /iitjhome/m23csa016/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/51c47430f97dd7c74aa1fa6825e68a813478097f/preprocessor_config.json
[INFO|image_processing_base.py:429] 2024-10-11 23:12:00,317 >> Image processor Qwen2VLImageProcessor {
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "max_pixels": 12845056,
    "min_pixels": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:2214] 2024-10-11 23:12:00,572 >> loading file vocab.json from cache at /iitjhome/m23csa016/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/51c47430f97dd7c74aa1fa6825e68a813478097f/vocab.json
[INFO|tokenization_utils_base.py:2214] 2024-10-11 23:12:00,572 >> loading file merges.txt from cache at /iitjhome/m23csa016/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/51c47430f97dd7c74aa1fa6825e68a813478097f/merges.txt
[INFO|tokenization_utils_base.py:2214] 2024-10-11 23:12:00,572 >> loading file tokenizer.json from cache at /iitjhome/m23csa016/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/51c47430f97dd7c74aa1fa6825e68a813478097f/tokenizer.json
[INFO|tokenization_utils_base.py:2214] 2024-10-11 23:12:00,572 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2214] 2024-10-11 23:12:00,572 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2214] 2024-10-11 23:12:00,572 >> loading file tokenizer_config.json from cache at /iitjhome/m23csa016/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/51c47430f97dd7c74aa1fa6825e68a813478097f/tokenizer_config.json
[INFO|tokenization_utils_base.py:2478] 2024-10-11 23:12:00,853 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|processing_utils.py:744] 2024-10-11 23:12:01,809 >> Processor Qwen2VLProcessor:
- image_processor: Qwen2VLImageProcessor {
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "max_pixels": 12845056,
    "min_pixels": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2-VL-7B-Instruct', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}

{
  "processor_class": "Qwen2VLProcessor"
}

10/11/2024 23:12:01 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
10/11/2024 23:12:01 - INFO - llamafactory.data.loader - Loading dataset women_group_attrs_10k.json...
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 7043 examples [00:00, 145319.18 examples/s]
Converting format of dataset (num_proc=16):   0%|          | 0/7043 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▌         | 433/7043 [00:00<00:01, 3378.64 examples/s]Converting format of dataset (num_proc=16):  74%|███████▍  | 5198/7043 [00:00<00:00, 26473.49 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 7043/7043 [00:00<00:00, 17933.35 examples/s]
10/11/2024 23:12:03 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>
10/11/2024 23:12:04 - INFO - llamafactory.data.loader - Loading dataset women_group_attrs_10k.json...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/7043 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▌         | 440/7043 [00:18<04:38, 23.70 examples/s]Running tokenizer on dataset (num_proc=16):  12%|█▏        | 880/7043 [00:18<01:48, 56.89 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▊        | 1320/7043 [00:19<01:01, 93.79 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▍       | 1760/7043 [00:19<00:34, 152.36 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███       | 2200/7043 [00:20<00:23, 209.55 examples/s]Running tokenizer on dataset (num_proc=16):  37%|███▋      | 2641/7043 [00:20<00:14, 302.57 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▎     | 3081/7043 [00:22<00:12, 307.84 examples/s]Running tokenizer on dataset (num_proc=16):  50%|████▉     | 3521/7043 [00:23<00:10, 350.31 examples/s]Running tokenizer on dataset (num_proc=16):  56%|█████▋    | 3962/7043 [00:23<00:07, 412.80 examples/s]Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 4403/7043 [00:23<00:04, 557.18 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 4843/7043 [00:25<00:04, 464.38 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 5283/7043 [00:25<00:02, 591.88 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 6163/7043 [00:25<00:01, 863.23 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 6603/7043 [00:26<00:00, 911.35 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 7043/7043 [00:27<00:00, 817.36 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 7043/7043 [00:27<00:00, 259.46 examples/s]
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 2082, 55856, 419, 42923, 285, 2168, 323, 10542, 25, 4946, 13597, 11, 3084, 11, 36153, 5118, 11, 36153, 1261, 98607, 151645, 198, 151644, 77091, 198, 64, 8447, 11, 64543, 3084, 11, 2326, 57314, 42375, 11, 5792, 151645]
inputs:
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|>Analyze this Kurtis image and identify: fit_shape, length, sleeve_length, sleeve_styling<|im_end|>
<|im_start|>assistant
a-line, calf length, three-quarter sleeves, regular<|im_end|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 64, 8447, 11, 64543, 3084, 11, 2326, 57314, 42375, 11, 5792, 151645]
labels:
a-line, calf length, three-quarter sleeves, regular<|im_end|>
[INFO|configuration_utils.py:672] 2024-10-11 23:12:32,632 >> loading configuration file config.json from cache at /iitjhome/m23csa016/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/51c47430f97dd7c74aa1fa6825e68a813478097f/config.json
[WARNING|modeling_rope_utils.py:379] 2024-10-11 23:12:32,632 >> Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
[INFO|configuration_utils.py:739] 2024-10-11 23:12:32,633 >> Model config Qwen2VLConfig {
  "_name_or_path": "Qwen/Qwen2-VL-7B-Instruct",
  "architectures": [
    "Qwen2VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 32768,
  "max_window_layers": 28,
  "model_type": "qwen2_vl",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.0",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "in_chans": 3,
    "model_type": "qwen2_vl",
    "spatial_patch_size": 14
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 152064
}

10/11/2024 23:12:32 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 8 bit with bitsandbytes.
Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}
10/11/2024 23:12:32 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 8 bit with bitsandbytes.
[INFO|modeling_utils.py:3726] 2024-10-11 23:12:33,649 >> loading weights file model.safetensors from cache at /iitjhome/m23csa016/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/51c47430f97dd7c74aa1fa6825e68a813478097f/model.safetensors.index.json
[INFO|modeling_utils.py:1622] 2024-10-11 23:12:33,651 >> Instantiating Qwen2VLForConditionalGeneration model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1099] 2024-10-11 23:12:33,652 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
[WARNING|logging.py:328] 2024-10-11 23:12:33,871 >> `Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:03<00:14,  3.74s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:03<00:15,  3.76s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:08<00:13,  4.37s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:08<00:13,  4.40s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:12<00:08,  4.31s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:12<00:08,  4.35s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:17<00:04,  4.27s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:17<00:04,  4.32s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:17<00:00,  2.96s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:17<00:00,  3.53s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [00:17<00:00,  2.99s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:17<00:00,  3.56s/it]
[INFO|modeling_utils.py:4568] 2024-10-11 23:12:51,943 >> All model checkpoint weights were used when initializing Qwen2VLForConditionalGeneration.

[INFO|modeling_utils.py:4576] 2024-10-11 23:12:51,943 >> All the weights of Qwen2VLForConditionalGeneration were initialized from the model checkpoint at Qwen/Qwen2-VL-7B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2VLForConditionalGeneration for predictions without further training.
[INFO|configuration_utils.py:1054] 2024-10-11 23:12:52,188 >> loading configuration file generation_config.json from cache at /iitjhome/m23csa016/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/51c47430f97dd7c74aa1fa6825e68a813478097f/generation_config.json
[INFO|configuration_utils.py:1099] 2024-10-11 23:12:52,189 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.01,
  "top_k": 1,
  "top_p": 0.001
}

10/11/2024 23:12:52 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
10/11/2024 23:12:52 - INFO - llamafactory.model.model_utils.visual - Casting multimodal projector outputs in torch.bfloat16.
10/11/2024 23:12:52 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
10/11/2024 23:12:52 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
10/11/2024 23:12:52 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
10/11/2024 23:12:52 - INFO - llamafactory.model.model_utils.misc - Found linear modules: o_proj,gate_proj,up_proj,v_proj,down_proj,k_proj,q_proj
10/11/2024 23:12:52 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
10/11/2024 23:12:52 - INFO - llamafactory.model.model_utils.visual - Casting multimodal projector outputs in torch.bfloat16.
10/11/2024 23:12:52 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
10/11/2024 23:12:52 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
10/11/2024 23:12:52 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
10/11/2024 23:12:52 - INFO - llamafactory.model.model_utils.misc - Found linear modules: down_proj,gate_proj,o_proj,q_proj,k_proj,v_proj,up_proj
10/11/2024 23:12:52 - INFO - llamafactory.model.loader - trainable params: 20,185,088 || all params: 8,311,560,704 || trainable%: 0.2429
10/11/2024 23:12:53 - INFO - llamafactory.model.loader - trainable params: 20,185,088 || all params: 8,311,560,704 || trainable%: 0.2429
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:667] 2024-10-11 23:12:53,027 >> Using auto half precision backend
[INFO|trainer.py:2243] 2024-10-11 23:12:54,225 >> ***** Running training *****
[INFO|trainer.py:2244] 2024-10-11 23:12:54,225 >>   Num examples = 6,338
[INFO|trainer.py:2245] 2024-10-11 23:12:54,225 >>   Num Epochs = 6
[INFO|trainer.py:2246] 2024-10-11 23:12:54,225 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2249] 2024-10-11 23:12:54,225 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:2250] 2024-10-11 23:12:54,225 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2251] 2024-10-11 23:12:54,225 >>   Total optimization steps = 1,188
[INFO|trainer.py:2252] 2024-10-11 23:12:54,234 >>   Number of trainable parameters = 20,185,088
  0%|          | 0/1188 [00:00<?, ?it/s]/iitjhome/m23csa016/.conda/envs/meesho/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/iitjhome/m23csa016/.conda/envs/meesho/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/iitjhome/m23csa016/.conda/envs/meesho/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/iitjhome/m23csa016/.conda/envs/meesho/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/iitjhome/m23csa016/.conda/envs/meesho/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/iitjhome/m23csa016/.conda/envs/meesho/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
  0%|          | 1/1188 [00:12<4:05:55, 12.43s/it]  0%|          | 2/1188 [00:22<3:34:53, 10.87s/it]  0%|          | 3/1188 [00:32<3:25:28, 10.40s/it]  0%|          | 4/1188 [00:41<3:21:04, 10.19s/it]  0%|          | 5/1188 [00:51<3:16:27,  9.96s/it]                                                  {'loss': 9.8482, 'grad_norm': 141.83409118652344, 'learning_rate': 0.005, 'epoch': 0.03, 'num_input_tokens_seen': 59584}
  0%|          | 5/1188 [00:51<3:16:27,  9.96s/it]  1%|          | 6/1188 [01:01<3:14:15,  9.86s/it]  1%|          | 7/1188 [01:11<3:14:50,  9.90s/it]  1%|          | 8/1188 [01:21<3:15:19,  9.93s/it]  1%|          | 9/1188 [01:31<3:15:12,  9.93s/it]  1%|          | 10/1188 [01:40<3:13:35,  9.86s/it]                                                   {'loss': 24.5458, 'grad_norm': 744.2838134765625, 'learning_rate': 0.005, 'epoch': 0.05, 'num_input_tokens_seen': 119136}
  1%|          | 10/1188 [01:40<3:13:35,  9.86s/it]  1%|          | 11/1188 [01:50<3:14:50,  9.93s/it]  1%|          | 12/1188 [02:01<3:18:06, 10.11s/it]  1%|          | 13/1188 [02:12<3:23:40, 10.40s/it]  1%|          | 14/1188 [02:22<3:24:09, 10.43s/it]  1%|▏         | 15/1188 [02:33<3:26:42, 10.57s/it]                                                   {'loss': 19.7122, 'grad_norm': 33.01877975463867, 'learning_rate': 0.005, 'epoch': 0.08, 'num_input_tokens_seen': 177248}
  1%|▏         | 15/1188 [02:33<3:26:42, 10.57s/it]  1%|▏         | 16/1188 [02:44<3:28:18, 10.66s/it]  1%|▏         | 17/1188 [02:55<3:29:31, 10.74s/it]  2%|▏         | 18/1188 [03:06<3:30:31, 10.80s/it]  2%|▏         | 19/1188 [03:17<3:29:20, 10.74s/it]  2%|▏         | 20/1188 [03:27<3:28:23, 10.70s/it]                                                   {'loss': 12.2772, 'grad_norm': 39.704471588134766, 'learning_rate': 0.005, 'epoch': 0.1, 'num_input_tokens_seen': 236064}
  2%|▏         | 20/1188 [03:27<3:28:23, 10.70s/it]  2%|▏         | 21/1188 [03:38<3:27:20, 10.66s/it]  2%|▏         | 22/1188 [03:48<3:26:45, 10.64s/it]  2%|▏         | 23/1188 [03:58<3:22:45, 10.44s/it]  2%|▏         | 24/1188 [04:09<3:23:10, 10.47s/it]  2%|▏         | 25/1188 [04:20<3:24:30, 10.55s/it]                                                   {'loss': 16.1886, 'grad_norm': 33.65108871459961, 'learning_rate': 0.005, 'epoch': 0.13, 'num_input_tokens_seen': 294944}
  2%|▏         | 25/1188 [04:20<3:24:30, 10.55s/it]  2%|▏         | 26/1188 [04:30<3:22:22, 10.45s/it]  2%|▏         | 27/1188 [04:40<3:20:01, 10.34s/it]  2%|▏         | 28/1188 [04:50<3:18:02, 10.24s/it]  2%|▏         | 29/1188 [05:00<3:17:17, 10.21s/it]  3%|▎         | 30/1188 [05:11<3:18:09, 10.27s/it]                                                   {'loss': 11.433, 'grad_norm': 29.787548065185547, 'learning_rate': 0.005, 'epoch': 0.15, 'num_input_tokens_seen': 354112}
  3%|▎         | 30/1188 [05:11<3:18:09, 10.27s/it]  3%|▎         | 31/1188 [05:21<3:18:55, 10.32s/it]  3%|▎         | 32/1188 [05:31<3:17:15, 10.24s/it]  3%|▎         | 33/1188 [05:41<3:17:22, 10.25s/it]  3%|▎         | 34/1188 [05:51<3:15:11, 10.15s/it]  3%|▎         | 35/1188 [06:01<3:15:38, 10.18s/it]                                                   {'loss': 5.8982, 'grad_norm': 20.88802719116211, 'learning_rate': 0.005, 'epoch': 0.18, 'num_input_tokens_seen': 413696}
  3%|▎         | 35/1188 [06:01<3:15:38, 10.18s/it]  3%|▎         | 36/1188 [06:12<3:16:45, 10.25s/it]  3%|▎         | 37/1188 [06:22<3:17:08, 10.28s/it]  3%|▎         | 38/1188 [06:33<3:17:58, 10.33s/it]  3%|▎         | 39/1188 [06:43<3:16:34, 10.26s/it]  3%|▎         | 40/1188 [06:53<3:16:32, 10.27s/it]                                                   {'loss': 4.7436, 'grad_norm': 11.928311347961426, 'learning_rate': 0.005, 'epoch': 0.2, 'num_input_tokens_seen': 473792}
  3%|▎         | 40/1188 [06:53<3:16:32, 10.27s/it]  3%|▎         | 41/1188 [07:03<3:16:17, 10.27s/it]  4%|▎         | 42/1188 [07:13<3:15:21, 10.23s/it]  4%|▎         | 43/1188 [07:24<3:14:54, 10.21s/it]  4%|▎         | 44/1188 [07:34<3:15:47, 10.27s/it]  4%|▍         | 45/1188 [07:44<3:15:59, 10.29s/it]                                                   {'loss': 4.1823, 'grad_norm': 12.933177947998047, 'learning_rate': 0.005, 'epoch': 0.23, 'num_input_tokens_seen': 532736}
  4%|▍         | 45/1188 [07:44<3:15:59, 10.29s/it]  4%|▍         | 46/1188 [07:55<3:16:19, 10.31s/it]  4%|▍         | 47/1188 [08:05<3:15:52, 10.30s/it]  4%|▍         | 48/1188 [08:15<3:15:54, 10.31s/it]  4%|▍         | 49/1188 [08:26<3:16:37, 10.36s/it]  4%|▍         | 50/1188 [08:37<3:18:21, 10.46s/it]                                                   {'loss': 3.6214, 'grad_norm': 9.398104667663574, 'learning_rate': 0.005, 'epoch': 0.25, 'num_input_tokens_seen': 592320}
  4%|▍         | 50/1188 [08:37<3:18:21, 10.46s/it]  4%|▍         | 51/1188 [08:48<3:21:07, 10.61s/it]  4%|▍         | 52/1188 [08:58<3:22:45, 10.71s/it]  4%|▍         | 53/1188 [09:10<3:24:59, 10.84s/it]  5%|▍         | 54/1188 [09:20<3:25:02, 10.85s/it]  5%|▍         | 55/1188 [09:31<3:25:57, 10.91s/it]                                                   {'loss': 3.0774, 'grad_norm': 8.103799819946289, 'learning_rate': 0.005, 'epoch': 0.28, 'num_input_tokens_seen': 652448}
  5%|▍         | 55/1188 [09:31<3:25:57, 10.91s/it]  5%|▍         | 56/1188 [09:42<3:25:06, 10.87s/it]  5%|▍         | 57/1188 [09:53<3:25:31, 10.90s/it]  5%|▍         | 58/1188 [10:04<3:26:12, 10.95s/it]  5%|▍         | 59/1188 [10:15<3:25:31, 10.92s/it]  5%|▌         | 60/1188 [10:26<3:26:45, 11.00s/it]                                                   {'loss': 2.9754, 'grad_norm': 6.198055267333984, 'learning_rate': 0.005, 'epoch': 0.3, 'num_input_tokens_seen': 712256}
  5%|▌         | 60/1188 [10:26<3:26:45, 11.00s/it]  5%|▌         | 61/1188 [10:37<3:24:08, 10.87s/it]  5%|▌         | 62/1188 [10:47<3:21:39, 10.75s/it]  5%|▌         | 63/1188 [10:58<3:22:15, 10.79s/it]  5%|▌         | 64/1188 [11:09<3:23:07, 10.84s/it]  5%|▌         | 65/1188 [11:20<3:23:58, 10.90s/it]                                                   {'loss': 2.7955, 'grad_norm': 5.934093475341797, 'learning_rate': 0.005, 'epoch': 0.33, 'num_input_tokens_seen': 769472}
  5%|▌         | 65/1188 [11:20<3:23:58, 10.90s/it]  6%|▌         | 66/1188 [11:31<3:24:35, 10.94s/it]  6%|▌         | 67/1188 [11:42<3:23:20, 10.88s/it]  6%|▌         | 68/1188 [11:53<3:23:52, 10.92s/it]  6%|▌         | 69/1188 [12:04<3:23:44, 10.92s/it]  6%|▌         | 70/1188 [12:15<3:24:53, 11.00s/it]                                                   {'loss': 2.8372, 'grad_norm': 5.563002586364746, 'learning_rate': 0.005, 'epoch': 0.35, 'num_input_tokens_seen': 828128}
  6%|▌         | 70/1188 [12:15<3:24:53, 11.00s/it]  6%|▌         | 71/1188 [12:26<3:24:48, 11.00s/it]  6%|▌         | 72/1188 [12:37<3:22:44, 10.90s/it]  6%|▌         | 73/1188 [12:47<3:21:06, 10.82s/it]  6%|▌         | 74/1188 [12:58<3:21:35, 10.86s/it]  6%|▋         | 75/1188 [13:09<3:20:33, 10.81s/it]                                                   {'loss': 2.6495, 'grad_norm': 2.9744272232055664, 'learning_rate': 0.005, 'epoch': 0.38, 'num_input_tokens_seen': 885024}
  6%|▋         | 75/1188 [13:09<3:20:33, 10.81s/it]  6%|▋         | 76/1188 [13:20<3:21:00, 10.85s/it]  6%|▋         | 77/1188 [13:31<3:21:29, 10.88s/it]  7%|▋         | 78/1188 [13:42<3:23:14, 10.99s/it]  7%|▋         | 79/1188 [13:53<3:21:27, 10.90s/it]  7%|▋         | 80/1188 [14:04<3:21:14, 10.90s/it]                                                   {'loss': 2.6765, 'grad_norm': 4.337910175323486, 'learning_rate': 0.005, 'epoch': 0.4, 'num_input_tokens_seen': 944384}
  7%|▋         | 80/1188 [14:04<3:21:14, 10.90s/it]  7%|▋         | 81/1188 [14:15<3:21:44, 10.93s/it]  7%|▋         | 82/1188 [14:26<3:21:54, 10.95s/it]  7%|▋         | 83/1188 [14:37<3:22:02, 10.97s/it]  7%|▋         | 84/1188 [14:48<3:23:05, 11.04s/it]  7%|▋         | 85/1188 [14:59<3:21:41, 10.97s/it]                                                   {'loss': 2.687, 'grad_norm': 7.662964344024658, 'learning_rate': 0.005, 'epoch': 0.43, 'num_input_tokens_seen': 1004608}
  7%|▋         | 85/1188 [14:59<3:21:41, 10.97s/it]  7%|▋         | 86/1188 [15:10<3:21:58, 11.00s/it]  7%|▋         | 87/1188 [15:21<3:19:57, 10.90s/it]  7%|▋         | 88/1188 [15:32<3:20:30, 10.94s/it]  7%|▋         | 89/1188 [15:42<3:19:39, 10.90s/it]  8%|▊         | 90/1188 [15:53<3:17:56, 10.82s/it]                                                   {'loss': 2.7116, 'grad_norm': 8.55607795715332, 'learning_rate': 0.005, 'epoch': 0.45, 'num_input_tokens_seen': 1063808}
  8%|▊         | 90/1188 [15:53<3:17:56, 10.82s/it]  8%|▊         | 91/1188 [16:04<3:17:12, 10.79s/it]  8%|▊         | 92/1188 [16:14<3:12:45, 10.55s/it]  8%|▊         | 93/1188 [16:24<3:13:12, 10.59s/it]  8%|▊         | 94/1188 [16:35<3:12:33, 10.56s/it]  8%|▊         | 95/1188 [16:45<3:12:06, 10.55s/it]                                                   {'loss': 2.7853, 'grad_norm': 5.461259841918945, 'learning_rate': 0.005, 'epoch': 0.48, 'num_input_tokens_seen': 1123648}
  8%|▊         | 95/1188 [16:45<3:12:06, 10.55s/it]  8%|▊         | 96/1188 [16:56<3:12:15, 10.56s/it]  8%|▊         | 97/1188 [17:07<3:12:48, 10.60s/it]  8%|▊         | 98/1188 [17:17<3:13:18, 10.64s/it]  8%|▊         | 99/1188 [17:29<3:15:53, 10.79s/it]  8%|▊         | 100/1188 [17:40<3:17:06, 10.87s/it]                                                    {'loss': 2.6516, 'grad_norm': 4.946508407592773, 'learning_rate': 0.005, 'epoch': 0.5, 'num_input_tokens_seen': 1182944}
  8%|▊         | 100/1188 [17:40<3:17:06, 10.87s/it][INFO|trainer.py:4021] 2024-10-11 23:30:34,804 >> 
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-10-11 23:30:34,804 >>   Num examples = 705
[INFO|trainer.py:4026] 2024-10-11 23:30:34,804 >>   Batch size = 4

  0%|          | 0/89 [00:00<?, ?it/s][A
  2%|▏         | 2/89 [00:01<01:05,  1.34it/s][A
  3%|▎         | 3/89 [00:02<01:29,  1.04s/it][A
  4%|▍         | 4/89 [00:04<01:43,  1.21s/it][A
  6%|▌         | 5/89 [00:05<01:47,  1.28s/it][A
  7%|▋         | 6/89 [00:07<01:49,  1.32s/it][A
  8%|▊         | 7/89 [00:08<01:49,  1.34s/it][A
  9%|▉         | 8/89 [00:10<01:50,  1.36s/it][A
 10%|█         | 9/89 [00:11<01:50,  1.38s/it][A
 11%|█         | 10/89 [00:12<01:47,  1.36s/it][A
 12%|█▏        | 11/89 [00:14<01:46,  1.37s/it][A
 13%|█▎        | 12/89 [00:15<01:48,  1.41s/it][A
 15%|█▍        | 13/89 [00:17<01:48,  1.42s/it][A
 16%|█▌        | 14/89 [00:18<01:47,  1.43s/it][A
 17%|█▋        | 15/89 [00:19<01:45,  1.42s/it][A
 18%|█▊        | 16/89 [00:21<01:43,  1.41s/it][A
 19%|█▉        | 17/89 [00:22<01:41,  1.41s/it][A
 20%|██        | 18/89 [00:24<01:38,  1.39s/it][A
 21%|██▏       | 19/89 [00:25<01:40,  1.44s/it][A
 22%|██▏       | 20/89 [00:27<01:39,  1.45s/it][A
 24%|██▎       | 21/89 [00:28<01:39,  1.46s/it][A
 25%|██▍       | 22/89 [00:30<01:37,  1.45s/it][A
 26%|██▌       | 23/89 [00:31<01:36,  1.47s/it][A
 27%|██▋       | 24/89 [00:33<01:37,  1.50s/it][A
 28%|██▊       | 25/89 [00:34<01:34,  1.48s/it][A
 29%|██▉       | 26/89 [00:35<01:31,  1.45s/it][A
 30%|███       | 27/89 [00:37<01:26,  1.39s/it][A
 31%|███▏      | 28/89 [00:38<01:26,  1.42s/it][A
 33%|███▎      | 29/89 [00:40<01:25,  1.43s/it][A
 34%|███▎      | 30/89 [00:41<01:22,  1.40s/it][A
 35%|███▍      | 31/89 [00:42<01:19,  1.37s/it][A
 36%|███▌      | 32/89 [00:44<01:17,  1.36s/it][A
 37%|███▋      | 33/89 [00:45<01:15,  1.35s/it][A
 38%|███▊      | 34/89 [00:46<01:14,  1.36s/it][A
 39%|███▉      | 35/89 [00:48<01:13,  1.35s/it][A
 40%|████      | 36/89 [00:49<01:11,  1.34s/it][A
 42%|████▏     | 37/89 [00:50<01:09,  1.34s/it][A
 43%|████▎     | 38/89 [00:52<01:07,  1.31s/it][A
 44%|████▍     | 39/89 [00:53<01:05,  1.32s/it][A
 45%|████▍     | 40/89 [00:54<01:04,  1.32s/it][A
 46%|████▌     | 41/89 [00:56<01:04,  1.34s/it][A
 47%|████▋     | 42/89 [00:57<01:03,  1.35s/it][A
 48%|████▊     | 43/89 [00:58<01:02,  1.35s/it][A
 49%|████▉     | 44/89 [01:00<01:00,  1.34s/it][A
 51%|█████     | 45/89 [01:01<00:59,  1.36s/it][A
 52%|█████▏    | 46/89 [01:02<00:57,  1.35s/it][A
 53%|█████▎    | 47/89 [01:04<00:54,  1.29s/it][A
 54%|█████▍    | 48/89 [01:05<00:54,  1.32s/it][A
 55%|█████▌    | 49/89 [01:06<00:51,  1.28s/it][A
 56%|█████▌    | 50/89 [01:07<00:50,  1.31s/it][A
 57%|█████▋    | 51/89 [01:09<00:49,  1.30s/it][A
 58%|█████▊    | 52/89 [01:10<00:48,  1.30s/it][A
 60%|█████▉    | 53/89 [01:11<00:46,  1.28s/it][A
 61%|██████    | 54/89 [01:13<00:45,  1.30s/it][A
 62%|██████▏   | 55/89 [01:14<00:44,  1.31s/it][A
 63%|██████▎   | 56/89 [01:15<00:43,  1.32s/it][A
 64%|██████▍   | 57/89 [01:17<00:42,  1.32s/it][A
 65%|██████▌   | 58/89 [01:18<00:40,  1.32s/it][A
 66%|██████▋   | 59/89 [01:19<00:39,  1.33s/it][A
 67%|██████▋   | 60/89 [01:21<00:37,  1.29s/it][A
 69%|██████▊   | 61/89 [01:22<00:35,  1.28s/it][A
 70%|██████▉   | 62/89 [01:23<00:35,  1.30s/it][A
 71%|███████   | 63/89 [01:24<00:33,  1.30s/it][A
 72%|███████▏  | 64/89 [01:26<00:32,  1.30s/it][A
 73%|███████▎  | 65/89 [01:27<00:31,  1.30s/it][A
 74%|███████▍  | 66/89 [01:28<00:30,  1.31s/it][A
 75%|███████▌  | 67/89 [01:30<00:28,  1.31s/it][A
 76%|███████▋  | 68/89 [01:31<00:27,  1.30s/it][A
 78%|███████▊  | 69/89 [01:32<00:26,  1.31s/it][A
 79%|███████▊  | 70/89 [01:34<00:25,  1.32s/it][A
 80%|███████▉  | 71/89 [01:35<00:23,  1.32s/it][A
 81%|████████  | 72/89 [01:36<00:21,  1.27s/it][A
 82%|████████▏ | 73/89 [01:37<00:20,  1.28s/it][A
 83%|████████▎ | 74/89 [01:39<00:19,  1.29s/it][A
 84%|████████▍ | 75/89 [01:40<00:17,  1.26s/it][A
 85%|████████▌ | 76/89 [01:41<00:16,  1.28s/it][A
 87%|████████▋ | 77/89 [01:43<00:15,  1.29s/it][A
 88%|████████▊ | 78/89 [01:44<00:14,  1.32s/it][A
 89%|████████▉ | 79/89 [01:45<00:13,  1.34s/it][A
 90%|████████▉ | 80/89 [01:47<00:11,  1.33s/it][A
 91%|█████████ | 81/89 [01:48<00:10,  1.33s/it][A
 92%|█████████▏| 82/89 [01:49<00:09,  1.32s/it][A
 93%|█████████▎| 83/89 [01:51<00:07,  1.33s/it][A
 94%|█████████▍| 84/89 [01:52<00:06,  1.30s/it][A
 96%|█████████▌| 85/89 [01:53<00:05,  1.31s/it][A
 97%|█████████▋| 86/89 [01:54<00:03,  1.31s/it][A
 98%|█████████▊| 87/89 [01:56<00:02,  1.30s/it][A
 99%|█████████▉| 88/89 [01:57<00:01,  1.32s/it][A
100%|██████████| 89/89 [01:58<00:00,  1.29s/it][A                                                    
                                               [A{'eval_loss': 2.6651723384857178, 'eval_runtime': 120.3352, 'eval_samples_per_second': 5.859, 'eval_steps_per_second': 0.74, 'epoch': 0.5, 'num_input_tokens_seen': 1182944}
  8%|▊         | 100/1188 [19:40<3:17:06, 10.87s/it]
100%|██████████| 89/89 [01:58<00:00,  1.29s/it][A
                                               [A  9%|▊         | 101/1188 [19:50<14:08:22, 46.83s/it]  9%|▊         | 102/1188 [20:01<10:49:26, 35.88s/it]  9%|▊         | 103/1188 [20:11<8:30:34, 28.23s/it]   9%|▉         | 104/1188 [20:21<6:52:23, 22.83s/it]  9%|▉         | 105/1188 [20:32<5:44:23, 19.08s/it]                                                    {'loss': 2.6899, 'grad_norm': 4.34428071975708, 'learning_rate': 0.005, 'epoch': 0.53, 'num_input_tokens_seen': 1241248}
  9%|▉         | 105/1188 [20:32<5:44:23, 19.08s/it]  9%|▉         | 106/1188 [20:42<4:55:48, 16.40s/it]  9%|▉         | 107/1188 [20:52<4:21:26, 14.51s/it]  9%|▉         | 108/1188 [21:02<3:59:42, 13.32s/it]  9%|▉         | 109/1188 [21:13<3:44:23, 12.48s/it]  9%|▉         | 110/1188 [21:24<3:35:09, 11.98s/it]                                                    {'loss': 2.6555, 'grad_norm': 3.321319341659546, 'learning_rate': 0.005, 'epoch': 0.55, 'num_input_tokens_seen': 1299904}
  9%|▉         | 110/1188 [21:24<3:35:09, 11.98s/it]  9%|▉         | 111/1188 [21:34<3:25:37, 11.46s/it]  9%|▉         | 112/1188 [21:45<3:20:51, 11.20s/it] 10%|▉         | 113/1188 [21:55<3:15:31, 10.91s/it] 10%|▉         | 114/1188 [22:05<3:13:31, 10.81s/it] 10%|▉         | 115/1188 [22:16<3:11:46, 10.72s/it]                                                    {'loss': 2.6352, 'grad_norm': 4.274823188781738, 'learning_rate': 0.005, 'epoch': 0.58, 'num_input_tokens_seen': 1359392}
 10%|▉         | 115/1188 [22:16<3:11:46, 10.72s/it] 10%|▉         | 116/1188 [22:27<3:11:07, 10.70s/it] 10%|▉         | 117/1188 [22:37<3:10:16, 10.66s/it] 10%|▉         | 118/1188 [22:48<3:08:42, 10.58s/it] 10%|█         | 119/1188 [22:58<3:08:17, 10.57s/it] 10%|█         | 120/1188 [23:09<3:07:29, 10.53s/it]                                                    {'loss': 2.5762, 'grad_norm': 5.143396854400635, 'learning_rate': 0.005, 'epoch': 0.61, 'num_input_tokens_seen': 1418464}
 10%|█         | 120/1188 [23:09<3:07:29, 10.53s/it] 10%|█         | 121/1188 [23:19<3:05:15, 10.42s/it] 10%|█         | 122/1188 [23:29<3:04:24, 10.38s/it] 10%|█         | 123/1188 [23:39<3:03:54, 10.36s/it] 10%|█         | 124/1188 [23:50<3:04:38, 10.41s/it] 11%|█         | 125/1188 [24:00<3:03:17, 10.35s/it]                                                    {'loss': 2.6315, 'grad_norm': 4.835544586181641, 'learning_rate': 0.005, 'epoch': 0.63, 'num_input_tokens_seen': 1475936}
 11%|█         | 125/1188 [24:00<3:03:17, 10.35s/it] 11%|█         | 126/1188 [24:11<3:04:10, 10.40s/it] 11%|█         | 127/1188 [24:21<3:04:31, 10.43s/it] 11%|█         | 128/1188 [24:31<3:04:05, 10.42s/it] 11%|█         | 129/1188 [24:41<3:00:50, 10.25s/it] 11%|█         | 130/1188 [24:52<3:01:03, 10.27s/it]                                                    {'loss': 2.8966, 'grad_norm': 12.689413070678711, 'learning_rate': 0.005, 'epoch': 0.66, 'num_input_tokens_seen': 1534720}
 11%|█         | 130/1188 [24:52<3:01:03, 10.27s/it] 11%|█         | 131/1188 [25:02<3:02:21, 10.35s/it] 11%|█         | 132/1188 [25:13<3:02:47, 10.39s/it] 11%|█         | 133/1188 [25:23<3:02:46, 10.39s/it] 11%|█▏        | 134/1188 [25:34<3:04:50, 10.52s/it] 11%|█▏        | 135/1188 [25:44<3:02:30, 10.40s/it]                                                    {'loss': 2.7919, 'grad_norm': 3.1781134605407715, 'learning_rate': 0.005, 'epoch': 0.68, 'num_input_tokens_seen': 1593344}
 11%|█▏        | 135/1188 [25:44<3:02:30, 10.40s/it] 11%|█▏        | 136/1188 [25:54<3:01:50, 10.37s/it] 12%|█▏        | 137/1188 [26:05<3:03:05, 10.45s/it] 12%|█▏        | 138/1188 [26:16<3:04:01, 10.52s/it] 12%|█▏        | 139/1188 [26:26<3:05:27, 10.61s/it] 12%|█▏        | 140/1188 [26:37<3:04:48, 10.58s/it]                                                    {'loss': 2.6114, 'grad_norm': 4.1187567710876465, 'learning_rate': 0.005, 'epoch': 0.71, 'num_input_tokens_seen': 1652832}
 12%|█▏        | 140/1188 [26:37<3:04:48, 10.58s/it] 12%|█▏        | 141/1188 [26:47<3:03:10, 10.50s/it] 12%|█▏        | 142/1188 [26:58<3:02:54, 10.49s/it] 12%|█▏        | 143/1188 [27:08<3:00:16, 10.35s/it] 12%|█▏        | 144/1188 [27:18<3:00:36, 10.38s/it] 12%|█▏        | 145/1188 [27:29<3:01:13, 10.42s/it]                                                    {'loss': 2.665, 'grad_norm': 6.694852828979492, 'learning_rate': 0.005, 'epoch': 0.73, 'num_input_tokens_seen': 1711552}
 12%|█▏        | 145/1188 [27:29<3:01:13, 10.42s/it] 12%|█▏        | 146/1188 [27:39<3:01:51, 10.47s/it] 12%|█▏        | 147/1188 [27:50<3:01:58, 10.49s/it] 12%|█▏        | 148/1188 [28:00<3:01:35, 10.48s/it] 13%|█▎        | 149/1188 [28:11<3:01:59, 10.51s/it] 13%|█▎        | 150/1188 [28:21<3:02:09, 10.53s/it]                                                    {'loss': 2.5912, 'grad_norm': 4.7394890785217285, 'learning_rate': 0.005, 'epoch': 0.76, 'num_input_tokens_seen': 1771264}
 13%|█▎        | 150/1188 [28:21<3:02:09, 10.53s/it] 13%|█▎        | 151/1188 [28:32<3:02:13, 10.54s/it] 13%|█▎        | 152/1188 [28:43<3:02:59, 10.60s/it] 13%|█▎        | 153/1188 [28:53<3:02:25, 10.58s/it] 13%|█▎        | 154/1188 [29:04<3:02:29, 10.59s/it] 13%|█▎        | 155/1188 [29:15<3:03:10, 10.64s/it]                                                    {'loss': 2.7574, 'grad_norm': 3.362499713897705, 'learning_rate': 0.005, 'epoch': 0.78, 'num_input_tokens_seen': 1830816}
 13%|█▎        | 155/1188 [29:15<3:03:10, 10.64s/it] 13%|█▎        | 156/1188 [29:25<3:01:54, 10.58s/it] 13%|█▎        | 157/1188 [29:35<3:00:41, 10.52s/it] 13%|█▎        | 158/1188 [29:46<3:00:59, 10.54s/it] 13%|█▎        | 159/1188 [29:57<3:00:30, 10.52s/it] 13%|█▎        | 160/1188 [30:07<3:01:53, 10.62s/it]                                                    {'loss': 2.7327, 'grad_norm': 3.683661937713623, 'learning_rate': 0.005, 'epoch': 0.81, 'num_input_tokens_seen': 1889056}
 13%|█▎        | 160/1188 [30:07<3:01:53, 10.62s/it] 14%|█▎        | 161/1188 [30:18<3:00:42, 10.56s/it] 14%|█▎        | 162/1188 [30:28<2:59:29, 10.50s/it] 14%|█▎        | 163/1188 [30:38<2:57:43, 10.40s/it] 14%|█▍        | 164/1188 [30:49<2:59:09, 10.50s/it] 14%|█▍        | 165/1188 [31:00<2:58:51, 10.49s/it]                                                    {'loss': 2.657, 'grad_norm': 4.862740993499756, 'learning_rate': 0.005, 'epoch': 0.83, 'num_input_tokens_seen': 1948800}
 14%|█▍        | 165/1188 [31:00<2:58:51, 10.49s/it] 14%|█▍        | 166/1188 [31:10<3:00:36, 10.60s/it] 14%|█▍        | 167/1188 [31:21<3:00:43, 10.62s/it] 14%|█▍        | 168/1188 [31:31<2:59:34, 10.56s/it] 14%|█▍        | 169/1188 [31:42<2:59:47, 10.59s/it] 14%|█▍        | 170/1188 [31:52<2:58:23, 10.51s/it]                                                    {'loss': 2.7216, 'grad_norm': 3.45954966545105, 'learning_rate': 0.005, 'epoch': 0.86, 'num_input_tokens_seen': 2008352}
 14%|█▍        | 170/1188 [31:52<2:58:23, 10.51s/it] 14%|█▍        | 171/1188 [32:03<2:57:30, 10.47s/it] 14%|█▍        | 172/1188 [32:13<2:56:19, 10.41s/it] 15%|█▍        | 173/1188 [32:24<2:56:04, 10.41s/it] 15%|█▍        | 174/1188 [32:34<2:56:43, 10.46s/it] 15%|█▍        | 175/1188 [32:44<2:55:06, 10.37s/it]                                                    {'loss': 2.6617, 'grad_norm': 5.416820049285889, 'learning_rate': 0.005, 'epoch': 0.88, 'num_input_tokens_seen': 2066432}
 15%|█▍        | 175/1188 [32:44<2:55:06, 10.37s/it] 15%|█▍        | 176/1188 [32:55<2:55:26, 10.40s/it] 15%|█▍        | 177/1188 [33:05<2:55:15, 10.40s/it] 15%|█▍        | 178/1188 [33:16<2:55:20, 10.42s/it] 15%|█▌        | 179/1188 [33:26<2:54:56, 10.40s/it] 15%|█▌        | 180/1188 [33:36<2:55:13, 10.43s/it]                                                    {'loss': 2.6391, 'grad_norm': 2.822561740875244, 'learning_rate': 0.005, 'epoch': 0.91, 'num_input_tokens_seen': 2125408}
 15%|█▌        | 180/1188 [33:36<2:55:13, 10.43s/it] 15%|█▌        | 181/1188 [33:47<2:53:53, 10.36s/it] 15%|█▌        | 182/1188 [33:57<2:55:04, 10.44s/it] 15%|█▌        | 183/1188 [34:08<2:54:55, 10.44s/it] 15%|█▌        | 184/1188 [34:18<2:53:57, 10.40s/it] 16%|█▌        | 185/1188 [34:29<2:54:20, 10.43s/it]                                                    {'loss': 2.6344, 'grad_norm': 3.6330196857452393, 'learning_rate': 0.005, 'epoch': 0.93, 'num_input_tokens_seen': 2184928}
 16%|█▌        | 185/1188 [34:29<2:54:20, 10.43s/it] 16%|█▌        | 186/1188 [34:39<2:56:04, 10.54s/it] 16%|█▌        | 187/1188 [34:50<2:54:39, 10.47s/it] 16%|█▌        | 188/1188 [35:00<2:55:00, 10.50s/it] 16%|█▌        | 189/1188 [35:11<2:55:43, 10.55s/it] 16%|█▌        | 190/1188 [35:21<2:55:29, 10.55s/it]                                                    {'loss': 2.6799, 'grad_norm': 3.573611259460449, 'learning_rate': 0.005, 'epoch': 0.96, 'num_input_tokens_seen': 2243744}
 16%|█▌        | 190/1188 [35:21<2:55:29, 10.55s/it] 16%|█▌        | 191/1188 [35:32<2:55:17, 10.55s/it] 16%|█▌        | 192/1188 [35:42<2:54:40, 10.52s/it] 16%|█▌        | 193/1188 [35:53<2:54:34, 10.53s/it] 16%|█▋        | 194/1188 [36:04<2:55:38, 10.60s/it] 16%|█▋        | 195/1188 [36:14<2:52:21, 10.41s/it]                                                    {'loss': 2.5348, 'grad_norm': 1.5214580297470093, 'learning_rate': 0.005, 'epoch': 0.98, 'num_input_tokens_seen': 2302816}
 16%|█▋        | 195/1188 [36:14<2:52:21, 10.41s/it] 16%|█▋        | 196/1188 [36:24<2:52:27, 10.43s/it] 17%|█▋        | 197/1188 [36:34<2:51:35, 10.39s/it] 17%|█▋        | 198/1188 [36:45<2:52:25, 10.45s/it] 17%|█▋        | 199/1188 [36:56<2:53:39, 10.54s/it] 17%|█▋        | 200/1188 [37:06<2:51:54, 10.44s/it]                                                    {'loss': 2.631, 'grad_norm': 2.6907706260681152, 'learning_rate': 0.005, 'epoch': 1.01, 'num_input_tokens_seen': 2361952}
 17%|█▋        | 200/1188 [37:06<2:51:54, 10.44s/it][INFO|trainer.py:4021] 2024-10-11 23:50:01,149 >> 
***** Running Evaluation *****
[INFO|trainer.py:4023] 2024-10-11 23:50:01,149 >>   Num examples = 705
[INFO|trainer.py:4026] 2024-10-11 23:50:01,149 >>   Batch size = 4

  0%|          | 0/89 [00:00<?, ?it/s][A
  2%|▏         | 2/89 [00:01<01:01,  1.41it/s][A
  3%|▎         | 3/89 [00:02<01:23,  1.03it/s][A
  4%|▍         | 4/89 [00:04<01:35,  1.13s/it][A
  6%|▌         | 5/89 [00:05<01:39,  1.19s/it][A
  7%|▋         | 6/89 [00:06<01:41,  1.22s/it][A
  8%|▊         | 7/89 [00:08<01:42,  1.25s/it][A
  9%|▉         | 8/89 [00:09<01:42,  1.27s/it][A
 10%|█         | 9/89 [00:10<01:42,  1.28s/it][A
 11%|█         | 10/89 [00:11<01:39,  1.26s/it][A
 12%|█▏        | 11/89 [00:13<01:38,  1.26s/it][A
 13%|█▎        | 12/89 [00:14<01:40,  1.31s/it][A
 15%|█▍        | 13/89 [00:15<01:40,  1.32s/it][A
 16%|█▌        | 14/89 [00:17<01:39,  1.32s/it][A
 17%|█▋        | 15/89 [00:18<01:37,  1.31s/it][A
 18%|█▊        | 16/89 [00:19<01:35,  1.30s/it][A
 19%|█▉        | 17/89 [00:21<01:33,  1.30s/it][A
 20%|██        | 18/89 [00:22<01:30,  1.28s/it][A
 21%|██▏       | 19/89 [00:23<01:33,  1.34s/it][A
 22%|██▏       | 20/89 [00:25<01:32,  1.34s/it][A
 24%|██▎       | 21/89 [00:26<01:31,  1.35s/it][A
 25%|██▍       | 22/89 [00:27<01:29,  1.34s/it][A
 26%|██▌       | 23/89 [00:29<01:29,  1.36s/it][A
 27%|██▋       | 24/89 [00:30<01:30,  1.39s/it][A
 28%|██▊       | 25/89 [00:32<01:28,  1.38s/it][A
 29%|██▉       | 26/89 [00:33<01:25,  1.35s/it][A
 30%|███       | 27/89 [00:34<01:19,  1.29s/it][A
 31%|███▏      | 28/89 [00:35<01:19,  1.31s/it][A
 33%|███▎      | 29/89 [00:37<01:19,  1.32s/it][A
 34%|███▎      | 30/89 [00:38<01:18,  1.32s/it][A
 35%|███▍      | 31/89 [00:39<01:16,  1.32s/it][A
 36%|███▌      | 32/89 [00:41<01:15,  1.33s/it][A
 37%|███▋      | 33/89 [00:42<01:13,  1.32s/it][A
 38%|███▊      | 34/89 [00:43<01:13,  1.34s/it][A
 39%|███▉      | 35/89 [00:45<01:12,  1.34s/it][A
 40%|████      | 36/89 [00:46<01:10,  1.33s/it][A
 42%|████▏     | 37/89 [00:47<01:09,  1.34s/it][A
 43%|████▎     | 38/89 [00:49<01:07,  1.32s/it][A
 44%|████▍     | 39/89 [00:50<01:06,  1.32s/it][A
 45%|████▍     | 40/89 [00:51<01:04,  1.32s/it][A
 46%|████▌     | 41/89 [00:53<01:04,  1.35s/it][A
 47%|████▋     | 42/89 [00:54<01:03,  1.35s/it][A
 48%|████▊     | 43/89 [00:55<01:02,  1.35s/it][A
 49%|████▉     | 44/89 [00:57<01:00,  1.35s/it][A
 51%|█████     | 45/89 [00:58<01:00,  1.36s/it][A
 52%|█████▏    | 46/89 [00:59<00:57,  1.35s/it][A
 53%|█████▎    | 47/89 [01:01<00:54,  1.29s/it][A
 54%|█████▍    | 48/89 [01:02<00:53,  1.31s/it][A
 55%|█████▌    | 49/89 [01:03<00:51,  1.28s/it][A
 56%|█████▌    | 50/89 [01:05<00:50,  1.30s/it][A
 57%|█████▋    | 51/89 [01:06<00:49,  1.30s/it][A
 58%|█████▊    | 52/89 [01:07<00:47,  1.29s/it][A
 60%|█████▉    | 53/89 [01:08<00:45,  1.28s/it][A
 61%|██████    | 54/89 [01:10<00:45,  1.30s/it][A
 62%|██████▏   | 55/89 [01:11<00:44,  1.31s/it][A
 63%|██████▎   | 56/89 [01:12<00:43,  1.32s/it][A
 64%|██████▍   | 57/89 [01:14<00:42,  1.32s/it][A
 65%|██████▌   | 58/89 [01:15<00:40,  1.32s/it][A
 66%|██████▋   | 59/89 [01:16<00:39,  1.32s/it][A
 67%|██████▋   | 60/89 [01:18<00:37,  1.29s/it][A
 69%|██████▊   | 61/89 [01:19<00:35,  1.27s/it][A
 70%|██████▉   | 62/89 [01:20<00:34,  1.29s/it][A
 71%|███████   | 63/89 [01:21<00:33,  1.29s/it][A
 72%|███████▏  | 64/89 [01:23<00:32,  1.29s/it][A
 73%|███████▎  | 65/89 [01:24<00:31,  1.29s/it][A
 74%|███████▍  | 66/89 [01:25<00:30,  1.30s/it][A
 75%|███████▌  | 67/89 [01:27<00:28,  1.31s/it][A
 76%|███████▋  | 68/89 [01:28<00:27,  1.30s/it][A
 78%|███████▊  | 69/89 [01:29<00:26,  1.31s/it][A
 79%|███████▊  | 70/89 [01:31<00:25,  1.32s/it][A
 80%|███████▉  | 71/89 [01:32<00:23,  1.32s/it][A
 81%|████████  | 72/89 [01:33<00:21,  1.28s/it][A
 82%|████████▏ | 73/89 [01:34<00:20,  1.29s/it][A
 83%|████████▎ | 74/89 [01:36<00:19,  1.30s/it][A
 84%|████████▍ | 75/89 [01:37<00:17,  1.26s/it][A
 85%|████████▌ | 76/89 [01:38<00:16,  1.28s/it][A
 87%|████████▋ | 77/89 [01:40<00:15,  1.28s/it][A
 88%|████████▊ | 78/89 [01:41<00:14,  1.32s/it][A
 89%|████████▉ | 79/89 [01:42<00:13,  1.34s/it][A
 90%|████████▉ | 80/89 [01:44<00:12,  1.34s/it][A
 91%|█████████ | 81/89 [01:45<00:10,  1.33s/it][A
 92%|█████████▏| 82/89 [01:46<00:09,  1.31s/it][A
 93%|█████████▎| 83/89 [01:48<00:07,  1.33s/it][A
 94%|█████████▍| 84/89 [01:49<00:06,  1.30s/it][A
 96%|█████████▌| 85/89 [01:50<00:05,  1.31s/it][A
 97%|█████████▋| 86/89 [01:51<00:03,  1.30s/it][A
 98%|█████████▊| 87/89 [01:53<00:02,  1.30s/it][A
 99%|█████████▉| 88/89 [01:54<00:01,  1.31s/it][A
100%|██████████| 89/89 [01:55<00:00,  1.28s/it][A                                                    
                                               [A{'eval_loss': 2.5773322582244873, 'eval_runtime': 117.173, 'eval_samples_per_second': 6.017, 'eval_steps_per_second': 0.76, 'epoch': 1.01, 'num_input_tokens_seen': 2361952}
 17%|█▋        | 200/1188 [39:03<2:51:54, 10.44s/it]
100%|██████████| 89/89 [01:55<00:00,  1.28s/it][A
                                               [A 17%|█▋        | 201/1188 [39:14<12:30:31, 45.62s/it] 17%|█▋        | 202/1188 [39:24<9:36:50, 35.10s/it] 